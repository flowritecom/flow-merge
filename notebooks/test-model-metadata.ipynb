{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Metadata List:\n",
      "Filename: .gitattributes, SHA: 11ad7efa24975ee4b0c3c3a38ed18737f0658a5f75a0a96787b576a78a023361, Size: 1519\n",
      "Filename: LICENSE, SHA: 41c2cf8c272f6fb0080a97cd9d9bd7d4604072b80a0b10e7d65ca26ef5000c0c, Size: 7281\n",
      "Filename: README.md, SHA: a1f74fb8b65e90381e38aecb1f97f035d58b886a8ba327373ee203cd15074703, Size: 2773\n",
      "Filename: config.json, SHA: fa5ce2577a12b2c41aab208242c3de5bdcc4309d147842ad88040b6d4349fc36, Size: 661\n",
      "Filename: generation_config.json, SHA: 8c970692323e3ea0e9b8b0a4dca79388d31226e41f83c9fd6014804280ebf6e8, Size: 138\n",
      "Filename: merges.txt, SHA: 599bab54075088774b1733fde865d5bd747cbcc7a547c5bc12610e874e26f5e3, Size: 1671839\n",
      "Filename: model.safetensors, SHA: a88bcf41b3fa9a20031b6b598abc11f694e35e0b5684d6e14dbe7e894ebbb080, Size: 1239173352\n",
      "Filename: tokenizer.json, SHA: f7c9b2dba4a296b1aa76c16a34b8225c0c118978400d4bb66bff0902d702f5b8, Size: 7028015\n",
      "Filename: tokenizer_config.json, SHA: 80d64fccf954f022eb43b7425eb45a133cbb92c76a5f24d4cea6dbe3d45548d9, Size: 1289\n",
      "Filename: vocab.json, SHA: ca10d7e9fb3ed18575dd1e277a2579c16d108e32f27439684afa0e10b1440910, Size: 2776833\n",
      "{'architectures': ['Qwen2ForCausalLM'], 'model_type': 'qwen2', 'tokenizer_config': {'bos_token': None, 'chat_template': \"{% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\\nYou are a helpful assistant<|im_end|>\\n' }}{% endif %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\", 'eos_token': '<|endoftext|>', 'pad_token': '<|endoftext|>', 'unk_token': None}}\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    %load_ext autoreload --quiet\n",
    "except ImportError:\n",
    "    %reload_ext autoreload \n",
    "%autoreload 2\n",
    "\n",
    "# Import necessary modules\n",
    "from pathlib import Path\n",
    "from flow_merge.lib.merge_settings import DirectorySettings\n",
    "from flow_merge.lib.model import Model\n",
    "from flow_merge.lib.model_metadata import ModelMetadataService\n",
    "#from flow_merge.lib.model_metadata import ModelMetadataService\n",
    "\n",
    "# Define the chunk size for hashing large files (used in model_metadata.py)\n",
    "CHUNK_SIZE = 64 * 1024  # 64KB\n",
    "\n",
    "# Specify the path or Hugging Face model ID to test\n",
    "path_or_id = \"Qwen/Qwen1.5-0.5B\"\n",
    "\n",
    "# Load the model metadata from the specified path or Hugging Face model ID\n",
    "metadata_service = ModelMetadataService()\n",
    "model_metadata = metadata_service.load_model_info(path_or_id)\n",
    "\n",
    "# Display the file metadata list to verify correct loading\n",
    "print(\"File Metadata List:\")\n",
    "for file_metadata in model_metadata.file_metadata_list:\n",
    "    print(f\"Filename: {file_metadata.filename}, SHA: {file_metadata.sha}, Size: {file_metadata.size}\")\n",
    "\n",
    "print(model_metadata.config)\n",
    "print(model_metadata.has_config)\n",
    "\n",
    "# Load the model using the Model class and the path or Hugging Face model ID\n",
    "model = Model.from_path(Path(path_or_id))\n",
    "\n",
    "# Display the model attributes to verify correct instantiation\n",
    "# print(\"\\nModel Attributes:\")\n",
    "# print(f\"ID: {model.id}\")\n",
    "# print(f\"Path: {model.path}\")\n",
    "# print(f\"Metadata: {model.metadata}\")\n",
    "# print(f\"File to Tensor Index: {model.file_to_tensor_index}\")\n",
    "# print(f\"Shards: {model.shards}\")\n",
    "# print(f\"Config: {model.config}\")\n",
    "# print(f\"Revision: {model.revision}\")\n",
    "# print(f\"Trust Remote Code: {model.trust_remote_code}\")\n",
    "\n",
    "## TODO: Check if we download config.json instead of using the one from metadata\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "from flow_merge.lib.model import Model\n",
    "\n",
    "mo = Model.from_path(Path(path_or_id))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flow-merge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
